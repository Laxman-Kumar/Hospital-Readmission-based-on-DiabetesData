{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "from pyspark.sql import functions as fn\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import feature\n",
    "# Funcionality for classification\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier, DecisionTreeClassifier,GBTClassifier\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "# grading import statements\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "# Correct Usage Example (pass ONLY the full file name):\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv\") # correct - pass ONLY the full file name  \n",
    "#   \n",
    "# Incorrect Usage Example\n",
    "#   file_name_to_load = get_training_filename(\"/sms_spam.csv\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv/\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"c:/users/will/data/sms_spam.csv\") incorrect -pass ONLY the full file name\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101766, 35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"W:/Syracuse/Assignements/BigData/FinalProject/Hospital-Readmission-based-on-Diabetes/Data/\"\n",
    "file_name = \"hospital_readmission_cleaned.csv\"\n",
    "diabetes_df = spark.read.csv(get_training_filename('hospital_readmission_cleaned.csv'), header=True, inferSchema=True)\n",
    "diabetes_df.toPandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns= ['race','diag_1','diag_2','diag_3','admission_type_name','dischage_disposition_name','admission_source_name',\n",
    "               'gender','age','max_glu_serum','A1Cresult','metformin','repaglinide',\n",
    "               'nateglinide','chlorpropamide','glimepiride','glipizide','glyburide','pioglitazone','rosiglitazone','acarbose',\n",
    "               'miglitol','insulin','glyburide-metformin','change','diabetesMed']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [feature.StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"keep\") for c in categorical_columns]\n",
    "\n",
    "# The encode of indexed values multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "pipeline1 = Pipeline(stages=indexers + encoders)\n",
    "#diabetes_df = pipeline1.fit(diabetes_df).transform(diabetes_df)\n",
    "#diabetes_df.toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnumeric_features = [t[0] for t in train.dtypes if t[1] == 'int']\\nnumeric_data = train.select(numeric_features).toPandas()\\naxs = pd.plotting.scatter_matrix(numeric_data, figsize=(15, 15));\\nn = len(numeric_data.columns)\\nfor i in range(n):\\n    v = axs[i, 0]\\n    v.yaxis.label.set_rotation(0)\\n    v.yaxis.label.set_ha('right')\\n    v.set_yticks(())\\n    h = axs[n-1, i]\\n    h.xaxis.label.set_rotation(90)\\n    h.set_xticks(())\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing\n",
    "train, test = diabetes_df.randomSplit([0.6, 0.4], 0)\n",
    "train, test2 = train.randomSplit([0.6, 0.4], 0)\n",
    "\n",
    "train.toPandas().shape\n",
    "'''\n",
    "numeric_features = [t[0] for t in train.dtypes if t[1] == 'int']\n",
    "numeric_data = train.select(numeric_features).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(15, 15));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipelines\n",
    "va = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "sc = feature.StandardScaler(withMean=True, inputCol='features',outputCol = 'zfeatures') \n",
    "\n",
    "pipeline2 = Pipeline(stages= [va,sc])\n",
    "#pipe_model = Pipeline(stages = [VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\"),\n",
    "#                                feature.StandardScaler(withMean=True, inputCol='features',outputCol = 'zfeatures')])\n",
    "#train = pipeline2.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression pipeline\n",
    "pipe_logit = Pipeline(stages = [pipeline1, pipeline2,LogisticRegression(labelCol='readmitted',featuresCol = 'zfeatures')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(5,5))\\nplt.plot([0, 1], [0, 1], 'r--')\\nplt.plot(fitted_pipe1.summary.roc.select('FPR').collect(),\\n         fitted_pipe1.summary.roc.select('TPR').collect())\\nplt.xlabel('FPR')\\nplt.ylabel('TPR')\\nplt.title('ROC')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitiing logitstic model\n",
    "#fitted_model = pipe_logit.fit(train)\n",
    "#fitted_model.transform(train).toPandas().head()\n",
    "\n",
    "fitted_pipe1 = pipe_logit.fit(train)\n",
    "'''\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(fitted_pipe1.summary.roc.select('FPR').collect(),\n",
    "         fitted_pipe1.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5964306563461123"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate logistic model\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\",rawPredictionCol='prediction', labelCol='readmitted')\n",
    "logit_predicted = fitted_pipe1.transform(test)\n",
    "evaluator.evaluate(logit_predicted)\n",
    "#fitted_model.transform(test).select(fn.avg(fn.expr('readmitted = prediction').cast('float'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|          Accuracy|            Recall|\n",
      "+------------------+------------------+\n",
      "|0.6010705212007598|0.5396185871608917|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_predicted.select(\n",
    "(fn.sum(fn.when(fn.col('readmitted')==fn.col('prediction'),1).otherwise(0))/fn.count(fn.col('readmitted'))).alias('Accuracy'),\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))/\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))+ \n",
    "fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==0),1).otherwise(0)))).alias('Recall')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest pipeline\n",
    "rf = RandomForestClassifier(labelCol=\"readmitted\", featuresCol=\"zfeatures\", numTrees=10)\n",
    "pipe_rf = Pipeline(stages = [pipeline1,pipeline2,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitiing RF model\n",
    "#fitted_model = pipe_rf.fit(train)\n",
    "#fitted_model.transform(train).toPandas().head()\n",
    "grid = (ParamGridBuilder().addGrid(rf.featureSubsetStrategy, ['auto']).addGrid(rf.impurity,['GINI']).addGrid(rf.numTrees, [80]).build())\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'readmitted',metricName = 'areaUnderROC')\n",
    "cv1 = CrossValidator(estimator=pipe_rf, estimatorParamMaps=grid, evaluator=evaluator, numFolds = 3, seed = 65)\n",
    "fitted_cv1 = cv1.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race',\n",
       " 'diag_1',\n",
       " 'diag_2',\n",
       " 'diag_3',\n",
       " 'admission_type_name',\n",
       " 'dischage_disposition_name',\n",
       " 'admission_source_name',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'time_in_hospital',\n",
       " 'num_lab_procedures',\n",
       " 'num_procedures',\n",
       " 'num_medications',\n",
       " 'number_outpatient',\n",
       " 'number_emergency',\n",
       " 'number_inpatient',\n",
       " 'number_diagnoses',\n",
       " 'max_glu_serum',\n",
       " 'A1Cresult',\n",
       " 'metformin',\n",
       " 'repaglinide',\n",
       " 'nateglinide',\n",
       " 'chlorpropamide',\n",
       " 'glimepiride',\n",
       " 'glipizide',\n",
       " 'glyburide',\n",
       " 'pioglitazone',\n",
       " 'rosiglitazone',\n",
       " 'acarbose',\n",
       " 'miglitol',\n",
       " 'insulin',\n",
       " 'glyburide-metformin',\n",
       " 'change',\n",
       " 'diabetesMed',\n",
       " 'readmitted']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5012928703077153\n"
     ]
    }
   ],
   "source": [
    "# validate random forest model\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\",rawPredictionCol='prediction', labelCol='readmitted')\n",
    "rf_predicted = fitted_cv1.transform(test)\n",
    "evaluator.evaluate(rf_predicted)\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(rf_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          Accuracy|              Recall|\n",
      "+------------------+--------------------+\n",
      "|0.5412159494670351|0.004851242862662602|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predicted.select(\n",
    "(fn.sum(fn.when(fn.col('readmitted')==fn.col('prediction'),1).otherwise(0))/fn.count(fn.col('readmitted'))).alias('Accuracy'),\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))/\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))+ \n",
    "fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==0),1).otherwise(0)))).alias('Recall')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(list(zip(train.columns[0:10], fitted_cv1.bestModel.stages[2].featureImportances.toArray())),\n",
    "           columns = ['feature', 'importance']).sort_values('importance', ascending = False)\n",
    "a.reset_index(drop=True, inplace=True)\n",
    "feature_list = a['feature'].head(10)\n",
    "\n",
    "bestPipeline = fitted_cv1.bestModel\n",
    "bestModel = bestPipeline.stages[2]\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "#feature_list= list(train.columns[0:-1])\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = fitted_model.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP pipeline\n",
    "layers = [4,5,5,2]\n",
    "pipe_mlp = Pipeline(stages = [pipe_model,MultilayerPerceptronClassifier(maxIter=100, layers=layers,labelCol = 'target',featuresCol = 'zfeatures', blockSize=128)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3678.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 641.0 failed 1 times, most recent failure: Lost task 4.0 in stage 641.0 (TID 3089, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\r\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:323)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:280)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\r\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:323)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:280)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\r\n\t... 18 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-d4e3641e9a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fitiing MLP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfitted_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfitted_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2142\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \"\"\"\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3678.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 641.0 failed 1 times, most recent failure: Lost task 4.0 in stage 641.0 (TID 3089, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\r\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:323)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:280)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\r\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\r\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:323)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:280)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\r\n\t... 18 more\r\n"
     ]
    }
   ],
   "source": [
    "# Fitiing MLP model\n",
    "fitted_model = pipe_mlp.fit(train)\n",
    "fitted_model.transform(test).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|avg(CAST((target = prediction) AS FLOAT))|\n",
      "+-----------------------------------------+\n",
      "|                       0.6189003436426117|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(labelCol=\"target\",featuresCol=\"zfeatures\")\n",
    "gbt_pipeline = Pipeline(stages=[pipe_model, gbt]).fit(train)\n",
    "gbt_pipeline.transform(test).select(fn.avg(fn.expr('target = prediction').cast('float'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"readmitted\", featuresCol=\"zfeatures\")\n",
    "dt_pipeline = Pipeline(stages=[pipeline1,pipeline2, dt]).fit(train)\n",
    "dt_prediction = dt_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5569429075392875\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\",rawPredictionCol='prediction', labelCol='readmitted')\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(dt_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|          Accuracy|             Recall|\n",
      "+------------------+-------------------+\n",
      "|0.5689795515650823|0.40956218103679826|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_prediction.select(\n",
    "(fn.sum(fn.when(fn.col('readmitted')==fn.col('prediction'),1).otherwise(0))/fn.count(fn.col('readmitted'))).alias('Accuracy'),\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))/\n",
    "(fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==1),1).otherwise(0))+ \n",
    "fn.sum(fn.when((fn.col('readmitted')==1) & (fn.col('prediction')==0),1).otherwise(0)))).alias('Recall')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetes_df = diabetes_df.withColumn(\"metformin\", diabetes_df[\"metformin\"].cast(IntegerType()))\n",
    "#diabetes_df = diabetes_df.withColumn(\"diag_2\", hosp_readmit_id_mapped_df.diag_2.cast('Int'))\n",
    "#print(diabetes_df.count())\n",
    "#diabetes_df = diabetes_df.na.drop()\n",
    "#diabetes_df.where(diabetes_df.metformin.isNull()).count()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "NULL VALUES IN \n",
    "diag_1\n",
    "diag_2\n",
    "diag_3\n",
    "max_glu_serum\n",
    "A1Cresult\n",
    "metformin_en\n",
    "repaglinide_en\n",
    "'''\n",
    "#diabetes_df = diabetes_df.withColumn('readmitted',when((diabetes_df.readmitted == '<30') | (diabetes_df.readmitted == '>30'),'YES').otherwise('NO'))\n",
    "#diabetes_df.toPandas().to_csv('Data/diabetes.csv')\n",
    "\n",
    "#52.169\n",
    "#discharge_en reduces to 50.73\n",
    "#source_en 55.24\n",
    "#time in hosp 55.95\n",
    "# number_outpatient 57.69\n",
    "#number_emergency 59.31\n",
    "# number_inpatient 63.46\n",
    "# number_diagnoses 63.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
